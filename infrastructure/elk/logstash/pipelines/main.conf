# Logstash Pipeline Configuration
# Version: 1.0.0
# Dependencies:
# - logstash-input-beats@7.17.0
# - logstash-filter-grok@4.6.0
# - logstash-filter-mutate@3.5.0
# - logstash-output-elasticsearch@11.0.0

# Input Section - Configure data ingestion sources
input {
  # Filebeat input for collecting logs from services
  beats {
    port => 5044
    ssl => true
    ssl_certificate => "/etc/logstash/certs/logstash.crt"
    ssl_key => "/etc/logstash/certs/logstash.key"
    ssl_verify_mode => "force_peer"
    include_codec_tag => true
    queue_size => 4096
    client_inactivity_timeout => 60
    receive_buffer_bytes => 32768
    max_connections => 10000
  }

  # TCP input for direct log shipping
  tcp {
    port => 5000
    codec => json
    ssl_enable => true
    ssl_cert => "/etc/logstash/certs/logstash.crt"
    ssl_key => "/etc/logstash/certs/logstash.key"
    ssl_verify => true
    queue_size => 4096
  }
}

# Filter Section - Process and enrich log data
filter {
  # Grok patterns for parsing structured logs
  grok {
    patterns_dir => ["/etc/logstash/patterns"]
    match => {
      "message" => [
        # Auth Service Pattern
        "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:log_level} \[%{DATA:thread}\] %{DATA:logger} - %{GREEDYDATA:log_message}",
        
        # Task Service Pattern
        "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:log_level} \[%{DATA:service}\] %{DATA:trace_id} - %{GREEDYDATA:log_message}",
        
        # File Service Pattern
        "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:log_level} \[%{DATA:service}\] %{DATA:request_id} - %{GREEDYDATA:log_message}"
      ]
    }
    pattern_definitions => {
      "TRACE_ID" => "[a-f0-9]{32}"
      "REQUEST_ID" => "req_[a-f0-9]{24}"
    }
    tag_on_failure => ["_grokparsefailure"]
    break_on_match => true
  }

  # Timestamp processing
  date {
    match => ["timestamp", "ISO8601"]
    target => "@timestamp"
    timezone => "UTC"
    remove_field => ["timestamp"]
  }

  # Field enrichment and type conversion
  mutate {
    add_field => {
      "environment" => "%{[kubernetes][namespace]}"
      "service" => "%{[kubernetes][container][name]}"
      "host" => "%{[kubernetes][node][name]}"
      "cluster" => "%{[kubernetes][cluster][name]}"
    }
    convert => {
      "response_time" => "integer"
      "status_code" => "integer"
    }
    remove_field => ["message"]
  }

  # Add metadata for log retention
  ruby {
    code => '
      event.set("retention_days", 
        case event.get("log_level")
        when "ERROR", "FATAL" then 1095  # 3 years for audit logs
        else 365  # 1 year for regular logs
        end
      )
    '
  }
}

# Output Section - Configure log storage destinations
output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "task-manager-logs-%{+YYYY.MM.dd}"
    user => "${ELASTIC_USERNAME}"
    password => "${ELASTIC_PASSWORD}"
    
    # Index template settings
    template_name => "task-manager"
    template_overwrite => true
    
    # Index Lifecycle Management
    ilm_enabled => true
    ilm_rollover_alias => "task-manager"
    ilm_pattern => "{now/d}-000001"
    ilm_policy => "task-manager-policy"
    
    # Performance optimization
    bulk_max_size => 5120
    flush_size => 500
    idle_flush_time => "1s"
    
    # Retry configuration
    retry_initial_interval => "2s"
    retry_max_interval => "64s"
    max_retries => 3
    retry_max_items => 5000
    
    # Security settings
    ssl => true
    ssl_certificate_verification => true
    
    # Pipeline workers
    workers => 4
    pipeline_batch_size => 250
    pipeline_batch_delay => 50
  }
}

# Performance Settings
pipeline.workers: 4
pipeline.batch.size: 250
pipeline.batch.delay: 50
queue.type: persisted
queue.max_bytes: 1gb